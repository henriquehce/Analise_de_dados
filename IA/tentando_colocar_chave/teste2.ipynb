{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f556fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                name                                       display_name                                                                                                                                                                                         description                                            supported_generation_methods\n",
      "                                          models/aqa Model that performs Attributed Question Answering.                                                                   Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.                                                          generateAnswer\n",
      "            models/deep-research-pro-preview-12-2025            Deep Research Pro Preview (Dec-12-2025)                                                                                                                                          Preview release (December 12th, 2025) of Deep Research Pro                                            generateContent, countTokens\n",
      "                             models/gemini-2.0-flash                                   Gemini 2.0 Flash                                                                                                                                                                                    Gemini 2.0 Flash generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "                         models/gemini-2.0-flash-001                               Gemini 2.0 Flash 001                                                          Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025. generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "        models/gemini-2.0-flash-exp-image-generation   Gemini 2.0 Flash (Image Generation) Experimental                                                                                                                                                    Gemini 2.0 Flash (Image Generation) Experimental                       generateContent, countTokens, bidiGenerateContent\n",
      "                        models/gemini-2.0-flash-lite                              Gemini 2.0 Flash-Lite                                                                                                                                                                               Gemini 2.0 Flash-Lite generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "                    models/gemini-2.0-flash-lite-001                          Gemini 2.0 Flash-Lite 001                                                                                                                                                             Stable version of Gemini 2.0 Flash-Lite generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "      models/gemini-2.5-computer-use-preview-10-2025            Gemini 2.5 Computer Use Preview 10-2025                                                                                                                                                             Gemini 2.5 Computer Use Preview 10-2025                                            generateContent, countTokens\n",
      "                             models/gemini-2.5-flash                                   Gemini 2.5 Flash                                                                   Stable version of Gemini 2.5 Flash, our mid-size multimodal model that supports up to 1 million tokens, released in June of 2025. generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "                       models/gemini-2.5-flash-image                                        Nano Banana                                                                                                                                                                      Gemini 2.5 Flash Preview Image                      generateContent, countTokens, batchGenerateContent\n",
      "                        models/gemini-2.5-flash-lite                              Gemini 2.5 Flash-Lite                                                                                                                                   Stable version of Gemini 2.5 Flash-Lite, released in July of 2025 generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "        models/gemini-2.5-flash-lite-preview-09-2025             Gemini 2.5 Flash-Lite Preview Sep 2025                                                                                                                                    Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "         models/gemini-2.5-flash-native-audio-latest               Gemini 2.5 Flash Native Audio Latest                                                                                                                                                     Latest release of Gemini 2.5 Flash Native Audio                                        countTokens, bidiGenerateContent\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025      Gemini 2.5 Flash Native Audio Preview 09-2025                                                                                                                                                       Gemini 2.5 Flash Native Audio Preview 09-2025                                        countTokens, bidiGenerateContent\n",
      "models/gemini-2.5-flash-native-audio-preview-12-2025      Gemini 2.5 Flash Native Audio Preview 12-2025                                                                                                                                                       Gemini 2.5 Flash Native Audio Preview 12-2025                                        countTokens, bidiGenerateContent\n",
      "             models/gemini-2.5-flash-preview-09-2025                  Gemini 2.5 Flash Preview Sep 2025                                                                                                                                                                   Gemini 2.5 Flash Preview Sep 2025 generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "                 models/gemini-2.5-flash-preview-tts                       Gemini 2.5 Flash Preview TTS                                                                                                                                                                        Gemini 2.5 Flash Preview TTS                                            countTokens, generateContent\n",
      "                               models/gemini-2.5-pro                                     Gemini 2.5 Pro                                                                                                                                                  Stable release (June 17th, 2025) of Gemini 2.5 Pro generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "                   models/gemini-2.5-pro-preview-tts                         Gemini 2.5 Pro Preview TTS                                                                                                                                                                          Gemini 2.5 Pro Preview TTS                      countTokens, generateContent, batchGenerateContent\n",
      "                       models/gemini-3-flash-preview                             Gemini 3 Flash Preview                                                                                                                                                                              Gemini 3 Flash Preview generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "                   models/gemini-3-pro-image-preview                                    Nano Banana Pro                                                                                                                                                                          Gemini 3 Pro Image Preview                      generateContent, countTokens, batchGenerateContent\n",
      "                         models/gemini-3-pro-preview                               Gemini 3 Pro Preview                                                                                                                                                                                Gemini 3 Pro Preview generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "                       models/gemini-3.1-pro-preview                             Gemini 3.1 Pro Preview                                                                                                                                                                              Gemini 3.1 Pro Preview generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "           models/gemini-3.1-pro-preview-customtools                Gemini 3.1 Pro Preview Custom Tools                                                                                                                                              Gemini 3.1 Pro Preview optimized for custom tool usage generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "                         models/gemini-embedding-001                               Gemini Embedding 001                                                                                                                                                      Obtain a distributed representation of a text.      embedContent, countTextTokens, countTokens, asyncBatchEmbedContent\n",
      "                              models/gemini-exp-1206                           Gemini Experimental 1206                                                                                                                                           Experimental release (March 25th, 2025) of Gemini 2.5 Pro generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "                          models/gemini-flash-latest                                Gemini Flash Latest                                                                                                                                                                      Latest release of Gemini Flash generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "                     models/gemini-flash-lite-latest                           Gemini Flash-Lite Latest                                                                                                                                                                 Latest release of Gemini Flash-Lite generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "                            models/gemini-pro-latest                                  Gemini Pro Latest                                                                                                                                                                        Latest release of Gemini Pro generateContent, countTokens, createCachedContent, batchGenerateContent\n",
      "               models/gemini-robotics-er-1.5-preview                     Gemini Robotics-ER 1.5 Preview                                                                                                                                                                      Gemini Robotics-ER 1.5 Preview                                            generateContent, countTokens\n",
      "                               models/gemma-3-12b-it                                        Gemma 3 12B                                                                                                                                                                                                                                                generateContent, countTokens\n",
      "                                models/gemma-3-1b-it                                         Gemma 3 1B                                                                                                                                                                                                                                                generateContent, countTokens\n",
      "                               models/gemma-3-27b-it                                        Gemma 3 27B                                                                                                                                                                                                                                                generateContent, countTokens\n",
      "                                models/gemma-3-4b-it                                         Gemma 3 4B                                                                                                                                                                                                                                                generateContent, countTokens\n",
      "                              models/gemma-3n-e2b-it                                       Gemma 3n E2B                                                                                                                                                                                                                                                generateContent, countTokens\n",
      "                              models/gemma-3n-e4b-it                                       Gemma 3n E4B                                                                                                                                                                                                                                                generateContent, countTokens\n",
      "                 models/imagen-4.0-fast-generate-001                                      Imagen 4 Fast                                                                                                                                                                 Vertex served Imagen 4.0 Fast model                                                                 predict\n",
      "                      models/imagen-4.0-generate-001                                           Imagen 4                                                                                                                                                                      Vertex served Imagen 4.0 model                                                                 predict\n",
      "            models/imagen-4.0-generate-preview-06-06                                 Imagen 4 (Preview)                                                                                                                                                                      Vertex served Imagen 4.0 model                                                                 predict\n",
      "                models/imagen-4.0-ultra-generate-001                                     Imagen 4 Ultra                                                                                                                                                                Vertex served Imagen 4.0 ultra model                                                                 predict\n",
      "      models/imagen-4.0-ultra-generate-preview-06-06                           Imagen 4 Ultra (Preview)                                                                                                                                                                Vertex served Imagen 4.0 ultra model                                                                 predict\n",
      "                      models/nano-banana-pro-preview                                    Nano Banana Pro                                                                                                                                                                          Gemini 3 Pro Image Preview                      generateContent, countTokens, batchGenerateContent\n",
      "                         models/veo-2.0-generate-001                                              Veo 2 Vertex served Veo 2 model. Access to this model requires billing to be enabled on the associated Google Cloud Platform account. Please visit https://console.cloud.google.com/billing to enable it.                                                      predictLongRunning\n",
      "                    models/veo-3.0-fast-generate-001                                         Veo 3 fast                                                                                                                                                                                          Veo 3 fast                                                      predictLongRunning\n",
      "                         models/veo-3.0-generate-001                                              Veo 3                                                                                                                                                                                               Veo 3                                                      predictLongRunning\n",
      "                models/veo-3.1-fast-generate-preview                                       Veo 3.1 fast                                                                                                                                                                                        Veo 3.1 fast                                                      predictLongRunning\n",
      "                     models/veo-3.1-generate-preview                                            Veo 3.1                                                                                                                                                                                             Veo 3.1                                                      predictLongRunning\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"GOOGLE_API_KEY\", \"\").strip()\n",
    "\n",
    "if not api_key:\n",
    "    print(\"GOOGLE_API_KEY não encontrada.\")\n",
    "else:\n",
    "    try:\n",
    "        genai.configure(api_key=api_key)\n",
    "\n",
    "        modelos = []\n",
    "        for m in genai.list_models():\n",
    "            metodos = getattr(m, \"supported_generation_methods\", []) or []\n",
    "            modelos.append({\n",
    "                \"name\": getattr(m, \"name\", \"\"),\n",
    "                \"display_name\": getattr(m, \"display_name\", \"\"),\n",
    "                \"description\": getattr(m, \"description\", \"\"),\n",
    "                \"supported_generation_methods\": \", \".join(metodos),\n",
    "            })\n",
    "\n",
    "        df_modelos = pd.DataFrame(modelos).sort_values(\"name\").reset_index(drop=True)\n",
    "        print(df_modelos.to_string(index=False))\n",
    "    except Exception as e:\n",
    "        print(f\"[Erro ao listar modelos] {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7f31f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo de log fechado no bloco finally.\n",
      "[TypeError] Tipos inválidos. Dica: use int/float. Detalhe: Parâmetros devem ser numéricos.\n",
      "Arquivo de log fechado no bloco finally.\n",
      "[ValueError] Verifique os valores informados: Valor inválido: 'a' não pode ser negativo neste exemplo.\n",
      "Arquivo de log fechado no bloco finally.\n",
      "[ZeroDivisionError] Divisão por zero não é permitida.\n",
      "Arquivo de log fechado no bloco finally.\n",
      "Total resenhas negativas: 2\n",
      "Texto unificado: Entrega atrasou e veio danificado. || Atendimento ruim e suporte não resolve.\n",
      "Categorias (bruto): Entrega, Qualidade do Produto, Atendimento ao Cliente\n",
      "Categorias (lista): ['Entrega', 'Dano', 'Atendimento', 'Su']\n",
      "JSONs convertidos para dict: [{'categoria': 'desconhecida', 'sentimento': 'negativo', 'gravidade': 'media', 'raw': '{'}, {'categoria': 'desconhecida', 'sentimento': 'negativo', 'gravidade': 'media', 'raw': '{'}]\n",
      "[Erro LLM local] ConnectionError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AC8B6F4E00>: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente'))\n",
      "Resposta local: \n",
      "\n",
      "=== RELATÓRIO FINAL ===\n",
      "Total negativas: 2\n",
      "Contagem de notas: {1: 1, 2: 1}\n",
      "Categorias detectadas:\n",
      "1. desconhecida (gravidade: media)\n",
      "2. desconhecida (gravidade: media)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from getpass import getpass\n",
    "\n",
    "# 1) Exemplo de tratamento de exceções (ValueError, TypeError, ZeroDivisionError + geral)\n",
    "def operacao_riscada(a, b, arquivo_log=\"log_temp.txt\"):\n",
    "    arquivo = None\n",
    "    try:\n",
    "        arquivo = open(arquivo_log, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "        if not isinstance(a, (int, float)) or not isinstance(b, (int, float)):\n",
    "            raise TypeError(\"Parâmetros devem ser numéricos.\")\n",
    "        if b == 0:\n",
    "            raise ZeroDivisionError(\"Divisão por zero não é permitida.\")\n",
    "        if a < 0:\n",
    "            raise ValueError(\"Valor inválido: 'a' não pode ser negativo neste exemplo.\")\n",
    "\n",
    "        resultado = a / b\n",
    "        arquivo.write(f\"Resultado: {resultado}\\n\")\n",
    "        return resultado\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"[ValueError] Verifique os valores informados: {e}\")\n",
    "    except TypeError as e:\n",
    "        print(f\"[TypeError] Tipos inválidos. Dica: use int/float. Detalhe: {e}\")\n",
    "    except ZeroDivisionError as e:\n",
    "        print(f\"[ZeroDivisionError] {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Erro inesperado] {type(e).__name__}: {e}\")\n",
    "    finally:\n",
    "        if arquivo:\n",
    "            arquivo.close()\n",
    "            print(\"Arquivo de log fechado no bloco finally.\")\n",
    "\n",
    "\n",
    "# Testes rápidos\n",
    "_ = operacao_riscada(10, 2)\n",
    "_ = operacao_riscada(\"10\", 2)\n",
    "_ = operacao_riscada(-1, 2)\n",
    "_ = operacao_riscada(10, 0)\n",
    "\n",
    "\n",
    "# 2) Filtrar DataFrame por análises negativas e unir resenhas\n",
    "df_neg = df[df[\"analise\"].str.lower().eq(\"negativa\")].copy()\n",
    "resenhas_negativas = df_neg[\"resenha\"].dropna().astype(str).tolist()\n",
    "texto_unificado = \" || \".join(resenhas_negativas)\n",
    "\n",
    "print(\"Total resenhas negativas:\", len(resenhas_negativas))\n",
    "print(\"Texto unificado:\", texto_unificado)\n",
    "\n",
    "\n",
    "# 3) Cliente LLM (Gemini 2.5 Flash) + chamada com parâmetros\n",
    "def chamar_gemini(prompt, model_name=\"gemini-2.5-flash\", temperature=0.2, top_p=0.9, max_tokens=300):\n",
    "    try:\n",
    "        chave = (api_key or os.getenv(\"GOOGLE_API_KEY\", \"\")).strip()\n",
    "        if not chave:\n",
    "            raise RuntimeError(\"GOOGLE_API_KEY não encontrada.\")\n",
    "\n",
    "        genai.configure(api_key=chave)\n",
    "        model = genai.GenerativeModel(model_name=model_name)\n",
    "        resposta = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "                \"max_output_tokens\": max_tokens,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        texto = getattr(resposta, \"text\", \"\")\n",
    "        if texto:\n",
    "            return texto.strip()\n",
    "\n",
    "        partes = []\n",
    "        for cand in getattr(resposta, \"candidates\", []) or []:\n",
    "            content = getattr(cand, \"content\", None)\n",
    "            for part in getattr(content, \"parts\", []) or []:\n",
    "                t = getattr(part, \"text\", \"\")\n",
    "                if t:\n",
    "                    partes.append(t)\n",
    "        return \"\\n\".join(partes).strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Erro LLM Google] {type(e).__name__}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Prompt inicial\n",
    "prompt_categorias = f\"\"\"\n",
    "Analise as resenhas negativas abaixo e extraia categorias principais de problema.\n",
    "Resenhas: {texto_unificado}\n",
    "Retorne as categorias separadas por vírgula.\n",
    "\"\"\"\n",
    "\n",
    "resposta_categorias = chamar_gemini(prompt_categorias)\n",
    "print(\"Categorias (bruto):\", resposta_categorias)\n",
    "\n",
    "\n",
    "# Prompt refinado: 1 palavra por categoria\n",
    "prompt_categorias_1_palavra = f\"\"\"\n",
    "Com base nas resenhas negativas abaixo, retorne apenas categorias de problema.\n",
    "Regras:\n",
    "- uma palavra por categoria\n",
    "- separadas por vírgula\n",
    "- sem explicações\n",
    "\n",
    "Resenhas: {texto_unificado}\n",
    "\"\"\"\n",
    "\n",
    "resposta_1_palavra = chamar_gemini(prompt_categorias_1_palavra)\n",
    "categorias_lista = [c.strip() for c in resposta_1_palavra.split(\",\") if c.strip()]\n",
    "print(\"Categorias (lista):\", categorias_lista)\n",
    "\n",
    "\n",
    "# 4) Extração de JSON por resenha (texto -> dict)\n",
    "def extrair_json_por_resenha(resenha):\n",
    "    prompt_json = f\"\"\"\n",
    "Classifique a resenha e devolva APENAS JSON válido neste formato:\n",
    "{{\"categoria\":\"...\", \"sentimento\":\"negativo\", \"gravidade\":\"baixa|media|alta\"}}\n",
    "\n",
    "Resenha: {resenha}\n",
    "\"\"\"\n",
    "    texto = chamar_gemini(prompt_json, max_tokens=120).strip()\n",
    "    texto = texto.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    try:\n",
    "        return json.loads(texto)\n",
    "    except json.JSONDecodeError:\n",
    "        inicio = texto.find(\"{\")\n",
    "        fim = texto.rfind(\"}\")\n",
    "        if inicio != -1 and fim != -1 and fim > inicio:\n",
    "            try:\n",
    "                return json.loads(texto[inicio:fim + 1])\n",
    "            except Exception:\n",
    "                pass\n",
    "        return {\n",
    "            \"categoria\": \"desconhecida\",\n",
    "            \"sentimento\": \"negativo\",\n",
    "            \"gravidade\": \"media\",\n",
    "            \"raw\": texto,\n",
    "        }\n",
    "\n",
    "\n",
    "json_dicts = [extrair_json_por_resenha(r) for r in resenhas_negativas]\n",
    "print(\"JSONs convertidos para dict:\", json_dicts)\n",
    "\n",
    "\n",
    "# 5) Modelo local (LM Studio/Ollama)\n",
    "def chamar_ollama(prompt, model=\"gemma3:1b\"):\n",
    "    # Exemplo de seleção/baixa:\n",
    "    #   ollama pull gemma3:1b\n",
    "    #   ollama serve\n",
    "    try:\n",
    "        url = \"http://localhost:11434/api/generate\"\n",
    "        payload = {\"model\": model, \"prompt\": prompt, \"stream\": False}\n",
    "        r = requests.post(url, json=payload, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        return r.json().get(\"response\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[Erro LLM local] {type(e).__name__}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "resposta_local = chamar_ollama(\"Resuma em uma frase: \" + texto_unificado)\n",
    "print(\"Resposta local:\", resposta_local)\n",
    "\n",
    "\n",
    "# 6) Processamento em lote + união do resultado final\n",
    "def processar_lote_resenhas(df_entrada, col_resenha=\"resenha\", col_analise=\"analise\", col_nota=\"nota\"):\n",
    "    df_n = df_entrada[df_entrada[col_analise].str.lower().eq(\"negativa\")].copy()\n",
    "    lista = df_n[col_resenha].dropna().astype(str).tolist()\n",
    "    contagem_notas = df_n[col_nota].value_counts(dropna=False).sort_index().to_dict()\n",
    "    resultados = [extrair_json_por_resenha(x) for x in lista]\n",
    "    return {\n",
    "        \"total_negativas\": len(lista),\n",
    "        \"contagem_notas\": contagem_notas,\n",
    "        \"resultados_json\": resultados,\n",
    "    }\n",
    "\n",
    "\n",
    "def unir_resultados_texto(resultado_lote):\n",
    "    linhas = [\n",
    "        f\"Total negativas: {resultado_lote['total_negativas']}\",\n",
    "        f\"Contagem de notas: {resultado_lote['contagem_notas']}\",\n",
    "        \"Categorias detectadas:\",\n",
    "    ]\n",
    "    for i, item in enumerate(resultado_lote[\"resultados_json\"], start=1):\n",
    "        linhas.append(f\"{i}. {item.get('categoria', 'desconhecida')} (gravidade: {item.get('gravidade', 'n/a')})\")\n",
    "    return \"\\n\".join(linhas)\n",
    "\n",
    "\n",
    "resultado_lote = processar_lote_resenhas(df)\n",
    "texto_final = unir_resultados_texto(resultado_lote)\n",
    "\n",
    "print(\"\\n=== RELATÓRIO FINAL ===\")\n",
    "print(texto_final)\n",
    "\n",
    "\n",
    "# 7) Ambiente virtual + IDE (referência)\n",
    "# python -m venv .venv\n",
    "# source .venv/bin/activate      # Linux/Mac\n",
    "# .venv\\Scripts\\activate         # Windows\n",
    "# pip install pandas requests google-generativeai\n",
    "# Use VS Code/PyCharm para executar o notebook/script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
